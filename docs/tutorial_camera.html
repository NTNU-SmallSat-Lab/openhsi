---

title: Quick Start Guide


keywords: fastai
sidebar: home_sidebar

summary: "Basic usage guide"
description: "Basic usage guide"
nb_path: "nbs/09_tutorial_camera.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/09_tutorial_camera.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To use an OpenHSI camera, you will need a settings <code>.json</code> file that describes how the camera is initialised and other details you can edit to suit your use case. You will also need a <code>.pkl</code> file that includes some arrays produced during calibration that allow the OpenHSI camera to do smile corrections, and conversions to radiance and reflectance.</p>
<p>For example, this is how you would use an OpenHSI camera (packaged with a Lucid sensor) and collect a hyperspectral datacube. The context manager automatically handles the initialisation and closing of the camera.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openhsi.cameras</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">json_path</span> <span class="o">=</span> <span class="s2">&quot;path_to_settings_file.json&quot;</span>
<span class="n">pkl_path</span>  <span class="o">=</span> <span class="s2">&quot;path_to_calibration_file.pkl&quot;</span>

<span class="k">with</span> <span class="n">LucidCamera</span><span class="p">(</span><span class="n">n_lines</span>        <span class="o">=</span> <span class="mi">1_000</span><span class="p">,</span> 
                 <span class="n">processing_lvl</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> 
                 <span class="n">pkl_path</span>       <span class="o">=</span> <span class="n">pkl_path</span><span class="p">,</span>
                 <span class="n">json_path</span>      <span class="o">=</span> <span class="n">json_path</span><span class="p">,</span>
                 <span class="n">exposure_ms</span>    <span class="o">=</span> <span class="mi">10</span>
                <span class="p">)</span> <span class="k">as</span> <span class="n">cam</span><span class="p">:</span>
    <span class="n">cam</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">plot_lib</span><span class="o">=</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> <span class="n">robust</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">fig</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we have a pushbroom sensor, we capture a line of spatial information at a time. Motion is required to obtain 2D spatial information and how many lines we collect is specified by <code>n_lines</code>. After <a href="/openhsi/cameras.html#LucidCamera.collect"><code>LucidCamera.collect</code></a> is run, the data is stored in a 3D numpy array <a href="/openhsi/cameras.html#LucidCamera.dc.data"><code>LucidCamera.dc.data</code></a> which is implemented as a circular buffer. The next section explains the <code>processing_lvl</code> parameter.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Processing-levels">Processing levels<a class="anchor-link" href="#Processing-levels"> </a></h2><p>The library comes with some predefined recipes you can use to output a datacube with the desired level of processing. Depending on your use case, you may want to use save raw data, or choose a faster binning scheme. The available options are listed below.</p>
<table>
<thead><tr>
<th style="text-align:right"><code>processing_lvl</code></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">-1</td>
<td>do not apply any transforms (<strong>default</strong>)</td>
</tr>
<tr>
<td style="text-align:right">0</td>
<td>raw digital numbers cropped to useable sensor area</td>
</tr>
<tr>
<td style="text-align:right">1</td>
<td>crop + fast smile</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td>crop + fast smile + fast binning</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td>crop + fast smile + slow binning</td>
</tr>
<tr>
<td style="text-align:right">4</td>
<td>crop + fast smile + fast binning + conversion to radiance in units of uW/cm^2/sr/nm</td>
</tr>
<tr>
<td style="text-align:right">5</td>
<td>crop + fast smile + radiance + fast binning</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td>crop + fast smile + fast binning + radiance + reflectance</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td>crop + fast smile + radiance + slow binning</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td>crop + fast smile + radiance + slow binning + reflectance</td>
</tr>
</tbody>
</table>
<p>Main difference between these is the order the transforms are used in the pipeline. This summaries the binning procedure and output:</p>
<table>
<thead><tr>
<th style="text-align:right"><code>processing_lvl</code></th>
<th>Binning</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right">-1,0,1</td>
<td>None</td>
<td>Digital Numbers</td>
</tr>
<tr>
<td style="text-align:right">2</td>
<td>Fast</td>
<td>Digital Numbers</td>
</tr>
<tr>
<td style="text-align:right">3</td>
<td>Slow</td>
<td>Digital Numbers</td>
</tr>
<tr>
<td style="text-align:right">4,5</td>
<td>Fast</td>
<td>Radiance (uW/cm^2/sr/nm)</td>
</tr>
<tr>
<td style="text-align:right">6</td>
<td>Fast</td>
<td>Reflectance</td>
</tr>
<tr>
<td style="text-align:right">7</td>
<td>Slow</td>
<td>Radiance (uW/cm^2/sr/nm)</td>
</tr>
<tr>
<td style="text-align:right">8</td>
<td>Slow</td>
<td>Reflectance</td>
</tr>
</tbody>
</table>
<p>Alternatively, you can supply a custom pipeline of transforms to 
<a href="/openhsi/cameras.html#LucidCamera.set_processing_lvl(custom_tfms = List[Callable[[np.ndarray],np.ndarray]]"><code>LucidCamera.set_processing_lvl(custom_tfms = List[Callable[[np.ndarray],np.ndarray]] )</code></a>).</p>
<h3 id="A-note-on-binning-schemes">A note on binning schemes<a class="anchor-link" href="#A-note-on-binning-schemes"> </a></h3><p>We provide a fast binning scheme that only involves one memory allocation to speed things up and it assumes that the wavelength profile along the spectral axis is linear. In practice, it is not exacly linear so we also provide a slow binning scheme that does it properly at the cost of requiring more memory allocations. We found that the extra time needed was around 2 ms on a Jetson Xavier board.</p>
<h3 id="Post-processing-Datacubes">Post-processing Datacubes<a class="anchor-link" href="#Post-processing-Datacubes"> </a></h3><p>If you are collecting raw data and want to post-process them into radiance or reflectance, you can use <a href="/openhsi/capture.html#ProcessDatacube"><code>ProcessDatacube</code></a> from the <a href="https://openhsi.github.io/openhsi/capture.html#ProcessDatacube">capture</a> module. 
For example, we have a datacube of digital numbers and we want to convert them to radiance. We need to pass a <code>processing_lvl</code> that includes the radiance conversion (so the the <code>dn2rad</code> method is initialised). Then we can pass in a list of transforms to <code>load_next_tfms</code> which will be applied to the whole datacube.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openhsi.capture</span> <span class="kn">import</span> <span class="n">ProcessDatacube</span>

<span class="n">dc2process</span> <span class="o">=</span> <span class="n">ProcessDatacube</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;path_to_datacube_file.nc&quot;</span><span class="p">,</span> <span class="n">processing_lvl</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                             <span class="n">json_path</span><span class="o">=</span><span class="n">json_path</span><span class="p">,</span> <span class="n">pkl_path</span><span class="o">=</span><span class="n">pkl_path</span><span class="p">)</span>
<span class="n">dc2process</span><span class="o">.</span><span class="n">load_next_tfms</span><span class="p">([</span><span class="n">proced_dc</span><span class="o">.</span><span class="n">dn2rad</span><span class="p">])</span>
<span class="n">dc2process</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>
<p>Just like the <a href="/openhsi/capture.html#SimulatedCamera"><code>SimulatedCamera</code></a>, you can then view your post-processed datacube by using</p>
<div class="highlight"><pre><span></span><span class="n">dc2process</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">hist_eq</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>or similar. More on visualisation in the next section.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Visualisation">Visualisation<a class="anchor-link" href="#Visualisation"> </a></h2><p>After collection, the datacube can be visualised as an RGB image using <a href="/openhsi/cameras.html#LucidCamera.show"><code>LucidCamera.show</code></a> which returns a figure object created using your chosen plotting backend <code>plot_lib</code>. The red, green, and blue wavelengths can be specified and the RGB channels will be chosen from the nearest wavelenth bands.</p>
<p>You may find that the contrast is low because of some outlier pixels from, for instance, specular reflection. To increase the contrast, we provide two options:</p>
<ul>
<li><code>robust</code>: rescale colours to the 2--98% percentile</li>
<li><code>hist_eq</code>: apply histogram equalisation
{% include note.html content='Default behaviour is no contrast adjustments.' %}</li>
</ul>
<p>If you just want to view a datacube without any cameras attached. You can do so using:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openhsi.data</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">dc</span> <span class="o">=</span> <span class="n">DataCube</span><span class="p">()</span>
<span class="n">dc</span><span class="o">.</span><span class="n">load_nc</span><span class="p">(</span><span class="s2">&quot;path_to_datacube_file.nc&quot;</span><span class="p">)</span>
<span class="n">dc</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">robust</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>If you want to interactively view your datacubes (tap and see spectra), you can do so using:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openhsi.atmos</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">dcv</span> <span class="o">=</span> <span class="n">DataCubeViewer</span><span class="p">(</span><span class="s2">&quot;path_to_datacube_file.nc&quot;</span><span class="p">)</span>
<span class="n">dcv</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Saving-datacubes">Saving datacubes<a class="anchor-link" href="#Saving-datacubes"> </a></h2><p>To save the datacube to NetCDF format (alongside an RGB picture), use <a href="/openhsi/cameras.html#LucidCamera.save"><code>LucidCamera.save</code></a>. For example:</p>
<div class="highlight"><pre><span></span><span class="n">cam</span><span class="o">.</span><span class="n">save</span><span class="p">(</span> <span class="n">save_dir</span> <span class="o">=</span> <span class="s2">&quot;beach_data&quot;</span> <span class="p">)</span>
</pre></div>
<p>will save a NetCDF file as f"beach_data/{current_date}/{current_datetime}.nc" <em>and also an RGB image</em> alongside. The save function also allows you to customise the file prefix and suffix. Preconfigured metadata can also be indicated to be saved into the NetCDF file. The camera temperature (in Celcius) and datatime for each camera frame is automatically included.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Getting-surface-reflectance">Getting surface reflectance<a class="anchor-link" href="#Getting-surface-reflectance"> </a></h2><p>Generally, processing to radiance is recommended. To process to reflectance in real-time, one requires knowledge of the atmospheric conditions at the time of collect. While this is facilitated by setting the <code>processing_lvl</code> to 6, internally, the algorithm relies on the pre-computed at sensor radiance saved in the calibration .pkl file (in the Python dictionary, "rad_fit" is the key). The other option is to use Empirical Line Calibration (see "Atmospheric Correction" in the sidebar).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Updating-the-radiative-transfer-model">Updating the radiative transfer model<a class="anchor-link" href="#Updating-the-radiative-transfer-model"> </a></h3><p>A radiative transfer model predicts the behaviour of sunlight as it enters the Earth's atmosphere. Some of the light will be absorbed, re-emitted, scattered, etc, from oxygen, nitrogen, carbon dioxide, methane, and aerosols to name a few. You don't want any clouds obstructing the sunlight.</p>
<p>Since the atmospheric conditions will change, you may need to recompute this every so often. Here is how to do it assuming your camera object is called <code>cam</code> from the example above:</p>
<div class="highlight"><pre><span></span><span class="c1"># camera initialised...</span>

<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">openhsi.atmos</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">Py6S</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Model6SV</span><span class="p">(</span><span class="n">lat</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;latitude&quot;</span><span class="p">],</span> <span class="n">lon</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;longitude&quot;</span><span class="p">],</span>
                 <span class="n">z_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;datetime_str&quot;</span><span class="p">],</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2"> %H:%M&quot;</span><span class="p">),</span>
                 <span class="n">station_num</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;radiosonde_station_num&quot;</span><span class="p">],</span> <span class="n">region</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;radiosonde_region&quot;</span><span class="p">],</span>
                 <span class="n">alt</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;altitude&quot;</span><span class="p">],</span> <span class="n">zen</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">azi</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="c1"># viewing zenith and azimuth angles</span>
                 <span class="n">aero_profile</span> <span class="o">=</span> <span class="n">AeroProfile</span><span class="o">.</span><span class="n">Maritime</span><span class="p">,</span>
                 <span class="n">wavelength_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">350</span><span class="p">,</span><span class="mi">900</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">2000</span><span class="p">),</span> <span class="c1"># choose larger range than sensor range</span>
                 <span class="n">sixs_path</span> <span class="o">=</span> <span class="n">cam</span><span class="o">.</span><span class="n">settings</span><span class="p">[</span><span class="s2">&quot;sixs_path&quot;</span><span class="p">])</span>

<span class="n">cam</span><span class="o">.</span><span class="n">calibration</span><span class="p">[</span><span class="s2">&quot;rad_fit&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">350</span><span class="p">,</span><span class="mi">900</span><span class="p">,</span><span class="n">num</span><span class="o">=</span><span class="mi">2000</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">radiance</span><span class="o">/</span><span class="mi">10</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;cubic&#39;</span><span class="p">)</span>

<span class="c1">#cam.dump(json_path,pkl_path) # update the settings and calibration files</span>
</pre></div>
<p>{% include important.html content='You will need the 6SV excutable somewhere on your system. You can specify the path to the executable with <code>sixs_path</code>. If you installed via <code>conda</code> you should be fine without specifying the path.' %}{% include important.html content='The 6SV model calculates radiance in units of (W/m^2/sr/μm), whereas the integrating sphere calibration is in (μW/cm^2/sr/nm) hence the extra divide by 10 in <code>model.radiance/10</code>. Use a <code>wavelength_array</code> that extends beyond the sensor range on both sides. ' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Empirical-Line-Calibration">Empirical Line Calibration<a class="anchor-link" href="#Empirical-Line-Calibration"> </a></h3><p>The ELC widget is defined in the <code>openhsi.atmos</code> module (<a href="https://openhsi.github.io/openhsi/atmos.html#ELC">link to documentation</a>). This method basically uses known spectral targets (typically one dark and one light) to extrapolate the reflectance for the other pixels. Users can draw several bounding boxes telling the ELC algorithm to use those pixels as the reference targets. Part of automatically identifying the spectral target, and thus an interactive widget, is to use a spectral matching technique. I use Spectral Angle Mapper and implemented it efficiently enough to be used interactively. 
{% include important.html content='Only ingests radiance datacubes. To view a digital number or reflectance datacube interactively, use <a href="/openhsi/atmos.html#DataCubeViewer"><code>DataCubeViewer</code></a> in the <code>openhsi.atmos</code> module.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openhsi.atmos</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">elc</span> <span class="o">=</span> <span class="n">ELC</span><span class="p">(</span><span class="n">nc_path</span><span class="o">=</span><span class="s2">&quot;path_to_radiance_datacube.nc&quot;</span><span class="p">,</span>
          <span class="n">speclib_path</span><span class="o">=</span><span class="s2">&quot;path_to_spectral_library.pkl&quot;</span><span class="p">,</span><span class="n">pkl_path</span><span class="o">=</span><span class="s2">&quot;path_to_camera_calibration_file.pkl&quot;</span><span class="p">)</span>
<span class="n">elc</span><span class="p">()</span>
</pre></div>
<p>The <code>speclib_path</code> parameter identifies the lab measured spectra of a few calibration tarps. This method also requires a radiance estimate <code>model_6SV</code> so we can spectrally match radiance from lab based reflectance - close enough is good enough.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Usage-Tips">Usage Tips<a class="anchor-link" href="#Usage-Tips"> </a></h2><p>Here are some tips from those who have used this library and camera in the field.</p>
<ul>
<li>Running the camera collect software from Jupyter Notebooks will impose some delays and slow down the frame rate. For best performance, run the camera collect from a script. </li>
</ul>

</div>
</div>
</div>
</div>
 

