{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/eway/Desktop/openhsi\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "import sys\n",
    "sys.executable\n",
    "sys.path\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "> Basic usage guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use an OpenHSI camera, you will need a settings `.json` file that describes how the camera is initialised and other details you can edit to suit your use case. You will also need a `.pkl` file that includes some arrays produced during calibration that allow the OpenHSI camera to do smile corrections, and conversions to radiance and reflectance. \n",
    "\n",
    "For example, this is how you would use an OpenHSI camera (packaged with a Lucid sensor) and collect a hyperspectral datacube. The context manager automatically handles the initialisation and closing of the camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from openhsi.cameras import *\n",
    "\n",
    "json_path = \"path_to_settings_file.json\"\n",
    "pkl_path  = \"path_to_calibration_file.pkl\"\n",
    "\n",
    "with LucidCamera(n_lines        = 1_000, \n",
    "                 processing_lvl = 2, \n",
    "                 pkl_path       = pkl_path,\n",
    "                 json_path      = json_path,\n",
    "                 exposure_ms    = 10\n",
    "                ) as cam:\n",
    "    cam.collect()\n",
    "    fig = cam.show(plot_lib=\"matplotlib\", robust=True)\n",
    "     \n",
    "fig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a pushbroom sensor, we capture a line of spatial information at a time. Motion is required to obtain 2D spatial information and how many lines we collect is specified by `n_lines`. After `LucidCamera.collect` is run, the data is stored in a 3D numpy array `LucidCamera.dc.data` which is implemented as a circular buffer. The next section explains the `processing_lvl` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing levels\n",
    "\n",
    "The library comes with some predefined recipes you can use to output a datacube with the desired level of processing. Depending on your use case, you may want to use save raw data, or choose a faster binning scheme. The available options are listed below. \n",
    "\n",
    "| `processing_lvl`  | Description  |\n",
    "|---:|---|\n",
    "| -1  | do not apply any transforms (**default**) |\n",
    "| 0  | raw digital numbers cropped to useable sensor area  |\n",
    "| 1  | crop + fast smile  |\n",
    "| 2  | crop + fast smile + fast binning   |\n",
    "| 3  | crop + fast smile + slow binning  |\n",
    "| 4  | crop + fast smile + fast binning + conversion to radiance in units of uW/cm^2/sr/nm  |\n",
    "| 5  | crop + fast smile + radiance + fast binning  |\n",
    "| 6  | crop + fast smile + fast binning + radiance + reflectance  |\n",
    "| 7  | crop + fast smile + radiance + slow binning  |\n",
    "| 8  | crop + fast smile + radiance + slow binning + reflectance  |\n",
    "\n",
    "Main difference between these is the order the transforms are used in the pipeline. This summaries the binning procedure and output:\n",
    "\n",
    "| `processing_lvl`  | Binning  |  Output  |\n",
    "|---:|---|---|\n",
    "| -1,0,1 | None | Digital Numbers |\n",
    "| 2  | Fast | Digital Numbers |\n",
    "| 3  | Slow | Digital Numbers |\n",
    "| 4,5  | Fast | Radiance (uW/cm^2/sr/nm) |\n",
    "| 6  | Fast | Reflectance |\n",
    "| 7  | Slow | Radiance (uW/cm^2/sr/nm) |\n",
    "| 8  | Slow | Reflectance |\n",
    "\n",
    "Alternatively, you can supply a custom pipeline of transforms to \n",
    "`LucidCamera.set_processing_lvl(custom_tfms = List[Callable[[np.ndarray],np.ndarray]] )`. \n",
    "\n",
    "### A note on binning schemes\n",
    "\n",
    "We provide a fast binning scheme that only involves one memory allocation to speed things up and it assumes that the wavelength profile along the spectral axis is linear. In practice, it is not exacly linear so we also provide a slow binning scheme that does it properly at the cost of requiring more memory allocations. We found that the extra time needed was around 2 ms on a Jetson Xavier board. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "After collection, the datacube can be visualised as an RGB image using `LucidCamera.show` which returns a figure object created using your chosen plotting backend `plot_lib`. The red, green, and blue wavelengths can be specified and the RGB channels will be chosen from the nearest wavelenth bands. \n",
    "\n",
    "You may find that the contrast is low because of some outlier pixels from, for instance, specular reflection. To increase the contrast, we provide two options:\n",
    "- `robust`: rescale colours to the 2--98% percentile\n",
    "- `hist_eq`: apply histogram equalisation\n",
    "\n",
    "> Note: Default behaviour is no contrast adjustments.\n",
    "\n",
    "\n",
    "If you just want to view a datacube without any cameras attached. You can do so using:\n",
    "```python\n",
    "from openhsi.data import *\n",
    "dc = DataCube()\n",
    "dc.load_nc(\"path_to_datacube_file.nc\")\n",
    "dc.show(robust=True)\n",
    "```\n",
    "\n",
    "If you want to interactively view your datacubes (tap and see spectra), you can do so using:\n",
    "```python\n",
    "from openhsi.atmos import *\n",
    "dcv = DataCubeViewer(\"path_to_datacube_file.nc\")\n",
    "dcv()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving datacubes\n",
    "\n",
    "To save the datacube to NetCDF format (alongside an RGB picture), use `LucidCamera.save`. For example:\n",
    "\n",
    "```python\n",
    "cam.save( save_dir = \"beach_data\" )\n",
    "```\n",
    "will save a NetCDF file as f\"beach_data/{current_date}/{current_datetime}.nc\" *and also an RGB image* alongside. The save function also allows you to customise the file prefix and suffix. Preconfigured metadata can also be indicated to be saved into the NetCDF file. The camera temperature (in Celcius) and datatime for each camera frame is automatically included. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting surface reflectance\n",
    "\n",
    "Generally, processing to radiance is recommended. To process to reflectance in real-time, one requires knowledge of the atmospheric conditions at the time of collect. While this is facilitated by setting the `processing_lvl` to 6, internally, the algorithm relies on the pre-computed at sensor radiance saved in the calibration .pkl file (in the Python dictionary, \"rad_fit\" is the key). The other option is to use Empirical Line Calibration (see \"Atmospheric Correction\" in the sidebar). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the radiative transfer model\n",
    "\n",
    "A radiative transfer model predicts the behaviour of sunlight as it enters the Earth's atmosphere. Some of the light will be absorbed, re-emitted, scattered, etc, from oxygen, nitrogen, carbon dioxide, methane, and aerosols to name a few. You don't want any clouds obstructing the sunlight. \n",
    "\n",
    "Since the atmospheric conditions will change, you may need to recompute this every so often. Here is how to do it assuming your camera object is called `cam` from the example above:\n",
    "\n",
    "```python\n",
    "\n",
    "# camera initialised...\n",
    "\n",
    "from datetime import datetime\n",
    "from openhsi.atmos import *\n",
    "from Py6S import *\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "model = Model6SV(lat = cam.settings[\"latitude\"], lon = cam.settings[\"longitude\"],\n",
    "                 z_time = datetime.strptime(cam.settings[\"datetime_str\"],\"%Y-%m-%d %H:%M\"),\n",
    "                 station_num = cam.settings[\"radiosonde_station_num\"], region = cam.settings[\"radiosonde_region\"],\n",
    "                 alt = cam.settings[\"altitude\"], zen = 0., azi = 0., # viewing zenith and azimuth angles\n",
    "                 aero_profile = AeroProfile.Maritime,\n",
    "                 wavelength_array = np.linspace(350,900,num=2000), # choose larger range than sensor range\n",
    "                 sixs_path = cam.settings[\"sixs_path\"])\n",
    "\n",
    "cam.calibration[\"rad_fit\"] = interp1d(np.linspace(350,900,num=2000), model.radiance/10, kind='cubic')\n",
    "\n",
    "#cam.dump(json_path,pkl_path) # update the settings and calibration files\n",
    "```\n",
    "\n",
    "> Important: You will need the 6SV excutable somewhere on your system. You can specify the path to the executable with `sixs_path`. If you installed via `conda` you should be fine without specifying the path.\n",
    "\n",
    "> Important: The 6SV model calculates radiance in units of (W/m^2/sr/μm), whereas the integrating sphere calibration is in (μW/cm^2/sr/nm) hence the extra divide by 10 in `model.radiance/10`. Use a `wavelength_array` that extends beyond the sensor range on both sides. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Line Calibration\n",
    "\n",
    "The ELC widget is defined in the `openhsi.atmos` module ([link to documentation](https://openhsi.github.io/openhsi/atmos.html#ELC)). This method basically uses known spectral targets (typically one dark and one light) to extrapolate the reflectance for the other pixels. Users can draw several bounding boxes telling the ELC algorithm to use those pixels as the reference targets. Part of automatically identifying the spectral target, and thus an interactive widget, is to use a spectral matching technique. I use Spectral Angle Mapper and implemented it efficiently enough to be used interactively. \n",
    "\n",
    "> Important: Only ingests radiance datacubes. To view a digital number or reflectance datacube interactively, use `DataCubeViewer` in the `openhsi.atmos` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from openhsi.atmos import *\n",
    "elc = ELC(nc_path=\"path_to_radiance_datacube.nc\",\n",
    "          speclib_path=\"path_to_spectral_library.pkl\",pkl_path=\"path_to_camera_calibration_file.pkl\")\n",
    "elc()\n",
    "```\n",
    "\n",
    "The `speclib_path` parameter identifies the lab measured spectra of a few calibration tarps. This method also requires a radiance estimate `model_6SV` so we can spectrally match radiance from lab based reflectance - close enough is good enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Tips\n",
    "\n",
    "Here are some tips from those who have used this library and camera in the field. \n",
    "\n",
    "- Running the camera collect software from Jupyter Notebooks will impose some delays and slow down the frame rate. For best performance, run the camera collect from a script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('3.8.3': pyenv)",
   "language": "python",
   "name": "python38364bit383pyenv4fc1801d817447c6b9c76f7bacd532a1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
